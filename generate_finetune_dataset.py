#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import time
import random
import threading
from queue import Queue
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI
import nest_asyncio

# Pour r√©soudre le probl√®me d'event loop dans certains environnements
nest_asyncio.apply()

# Configuration g√©n√©rale
OUTPUT_FILE = "./data/generated_examples.json"
PROCESSED_FILE = "./data/processed_dataset.jsonl"  # Format pour fine-tuning
OPENAI_API_KEY = "sk-votre_cle_api_ici"  # √Ä remplacer par votre cl√© API
NUM_EXAMPLES = 500  # Ajust√© √† 500 comme demand√©
BATCH_SIZE = 5
MAX_CONCURRENT = 5

# Constantes pour la m√©moire hi√©rarchique
NIVEAUX = {
    1: "√©change",      # Conversations directes
    2: "r√©sum√©",       # R√©sum√©s des conversations
    3: "synth√®se",     # Synth√®ses des r√©sum√©s
    4: "conscience",   # R√©flexions sur l'identit√© et l'√©volution
    5: "m√©tacognition" # R√©flexion sur les processus de pens√©e
}

# Sch√©ma de pens√©e structur√©
SCHEMA_DE_PENSEE = """
# TEMPLATE DE PENS√âE STRUCTUR√âE

## 1. ANALYSE INITIALE
- D√©finir clairement la requ√™te ou le probl√®me
- Identifier les informations pertinentes dans le contexte
- D√©terminer les connaissances pr√©alables n√©cessaires

## 2. D√âCOMPOSITION
- Diviser le probl√®me en sous-probl√®mes g√©rables
- Organiser les composants dans un ordre logique
- D√©terminer les d√©pendances entre les composants

## 3. EXPLORATION DES SOLUTIONS
- G√©n√©rer plusieurs approches potentielles
- √âvaluer les avantages et inconv√©nients de chaque approche
- S√©lectionner l'approche la plus adapt√©e

## 4. IMPL√âMENTATION
- D√©velopper la solution √©tape par √©tape
- V√©rifier la coh√©rence √† chaque √©tape
- Ajuster la solution si n√©cessaire

## 5. R√âVISION ET VALIDATION
- √âvaluer la solution par rapport √† la requ√™te initiale
- V√©rifier l'exactitude et la compl√©tude
- Identifier les am√©liorations potentielles

## 6. FORMULATION DE LA R√âPONSE
- Structurer la r√©ponse de mani√®re claire et logique
- Adapter le niveau de d√©tail au contexte
- Pr√©senter la r√©ponse de mani√®re concise et informative
"""

# Liste de domaines pour varier les exemples
DOMAINS = [
    "Informatique et programmation",
    "Science et technologie",
    "Math√©matiques et logique",
    "Philosophie et √©thique",
    "Histoire et civilisations",
    "√âconomie et finance",
    "Psychologie et comportement humain",
    "Sant√© et m√©decine",
    "Art et litt√©rature",
    "Environnement et d√©veloppement durable",
    "Politique et g√©opolitique",
    "√âducation et apprentissage",
    "Droit et justice",
    "Business et entrepreneuriat",
    "Communication et langages"
]

class ParallelGenerator:
    def __init__(self, api_key, num_examples, batch_size, max_concurrent):
        self.api_key = api_key
        self.num_examples = num_examples
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent
        self.examples_queue = Queue()
        self.all_examples = []
        self.lock = threading.Lock()
        self.save_path = OUTPUT_FILE
        self.client = OpenAI(api_key=api_key)
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent)

    def generate_batch(self, batch_id):
        current_batch_size = min(self.batch_size, self.num_examples - batch_id * self.batch_size)
        if current_batch_size <= 0:
            return []

        domain = random.choice(DOMAINS)
        print(f"üîÑ G√©n√©ration du lot {batch_id+1} - Domaine: {domain}...")

        prompt = f"""
G√©n√®re {current_batch_size} exemples d'entra√Ænement pour un LLM qui apprend √† utiliser un sch√©ma de pens√©e structur√© et √† interagir avec une m√©moire hi√©rarchique.

Chaque exemple doit contenir:
1. Une question complexe dans le domaine: {domain}
2. Un contexte de m√©moire simul√© avec:
   - 1-2 √©changes r√©cents (niveau 1)
   - Un r√©sum√© de conversation (niveau 2)
   - Une synth√®se (niveau 3, optionnel)
   - Un √©tat de conscience (niveau 4, optionnel)
3. Pour chaque √©tape du sch√©ma de pens√©e, un paragraphe appliquant cette √©tape √† la question en int√©grant le contexte m√©moire
4. Une r√©ponse finale compl√®te qui d√©montre l'utilisation fluide de la m√©moire hi√©rarchique

SCH√âMA DE PENS√âE:
{SCHEMA_DE_PENSEE}

NIVEAUX DE M√âMOIRE:
1: √©change - Conversations directes
2: r√©sum√© - R√©sum√©s des conversations
3: synth√®se - Synth√®ses des r√©sum√©s
4: conscience - R√©flexions sur l'identit√©
5: m√©tacognition - R√©flexion sur les processus de pens√©e

Format de r√©ponse:
{{
  "examples": [
    {{
      "question": "[question complexe]",
      "memory_context": {{
        "exchanges": [
          {{"prompt": "[question pr√©c√©dente]", "response": "[r√©ponse pr√©c√©dente]"}},
          {{"prompt": "[autre question]", "response": "[autre r√©ponse]"}}
        ],
        "summary": "[r√©sum√© de conversation]",
        "synthesis": "[synth√®se optionnelle]",
        "consciousness": "[√©tat de conscience optionnel]"
      }},
      "analysis": "[paragraphe d'analyse int√©grant la m√©moire]",
      "decomposition": "[paragraphe de d√©composition]",
      "exploration": "[paragraphe d'exploration des solutions]",
      "implementation": "[paragraphe d'impl√©mentation]",
      "revision": "[paragraphe de r√©vision et validation]",
      "formulation": "[paragraphe de formulation de r√©ponse]",
      "answer": "[r√©ponse finale d√©taill√©e int√©grant naturellement la m√©moire]"
    }}
  ]
}}

Renvoie uniquement un objet JSON valide avec {current_batch_size} exemples, sans texte suppl√©mentaire.
"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Tu es un assistant sp√©cialis√© dans la cr√©ation de donn√©es d'entra√Ænement pour LLM avec m√©moire hi√©rarchique."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )
            content = response.choices[0].message.content
            batch_examples = json.loads(content).get("examples", [])
            valid_examples = []
            for example in batch_examples:
                schema_parts = ["question", "memory_context", "analysis", "decomposition",
                                "exploration", "implementation", "revision", "formulation", "answer"]
                missing_parts = [part for part in schema_parts if part not in example or not example[part]]
                if "memory_context" in example and isinstance(example["memory_context"], dict):
                    if "exchanges" not in example["memory_context"] or not example["memory_context"]["exchanges"]:
                        missing_parts.append("memory_context.exchanges")
                    if "summary" not in example["memory_context"] or not example["memory_context"]["summary"]:
                        missing_parts.append("memory_context.summary")
                else:
                    missing_parts.append("memory_context structure")
                if not missing_parts:
                    valid_examples.append(example)
            
            print(f"‚úÖ Lot {batch_id+1}: {len(valid_examples)}/{current_batch_size} exemples valides g√©n√©r√©s")
            with self.lock:
                self.all_examples.extend(valid_examples)
                for ex in valid_examples:
                    self.examples_queue.put(ex)
                with open(self.save_path, "w") as f:
                    json.dump(self.all_examples, f, indent=2)
            return valid_examples
        except Exception as e:
            print(f"‚ùå Erreur dans le lot {batch_id+1}: {str(e)}")
            return []

    def run_generation(self):
        """Lance la g√©n√©ration parall√®le de tous les exemples"""
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)
        
        start_time = time.time()
        print(f"üöÄ D√©but de la g√©n√©ration de {self.num_examples} exemples...")
        
        # Nombre de lots √† traiter
        num_batches = (self.num_examples + self.batch_size - 1) // self.batch_size
        
        # G√©n√©ration des lots en parall√®le
        futures = []
        for batch_id in range(num_batches):
            future = self.executor.submit(self.generate_batch, batch_id)
            futures.append(future)
        
        # Attendre que tous les lots soient trait√©s
        generated_count = 0
        for future in futures:
            examples = future.result()
            generated_count += len(examples)
        
        end_time = time.time()
        print(f"‚ú® G√©n√©ration termin√©e! {generated_count}/{self.num_examples} exemples g√©n√©r√©s en {end_time - start_time:.2f} secondes")
        
        return self.all_examples

    def process_to_finetune_format(self, output_file=PROCESSED_FILE):
        """Convertit les exemples g√©n√©r√©s au format d'entra√Ænement pour DeepSeek"""
        if not self.all_examples:
            print("‚ùå Aucun exemple √† traiter")
            return
        
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        finetune_data = []
        
        for example in self.all_examples:
            # Construire le contexte de m√©moire
            memory_context = ""
            if "memory_context" in example:
                if "exchanges" in example["memory_context"] and example["memory_context"]["exchanges"]:
                    memory_context += "CONVERSATIONS PR√âC√âDENTES:\n"
                    for exchange in example["memory_context"]["exchanges"]:
                        memory_context += f"Question: {exchange['prompt']}\n"
                        memory_context += f"R√©ponse: {exchange['response']}\n\n"
                
                if "summary" in example["memory_context"] and example["memory_context"]["summary"]:
                    memory_context += f"R√âSUM√â:\n{example['memory_context']['summary']}\n\n"
                
                if "synthesis" in example["memory_context"] and example["memory_context"]["synthesis"]:
                    memory_context += f"SYNTH√àSE:\n{example['memory_context']['synthesis']}\n\n"
                
                if "consciousness" in example["memory_context"] and example["memory_context"]["consciousness"]:
                    memory_context += f"CONSCIENCE:\n{example['memory_context']['consciousness']}\n\n"
            
            # Construire le message utilisateur
            user_message = example.get("question", "")
            
            # Construire la r√©ponse concise (sans processus de pens√©e)
            assistant_response = example.get("answer", "")
            
            # Format pour DeepSeek
            finetune_item = {
                "messages": [
                    {
                        "role": "system",
                        "content": """Tu es Ren√©e, une assistante IA francophone qui suit ces r√®gles strictes:
1. R√©ponds UNIQUEMENT en utilisant les informations des CONTEXTES ci-dessous
2. Si l'information n'est pas dans les CONTEXTES, r√©ponds simplement "Information non disponible"
3. Sois TR√àS CONCISE (20-30 mots maximum)
4. Ne g√©n√®re PAS de section <think> - r√©ponds directement
5. Utilise un langage simple et direct

CONTEXTE DE LA M√âMOIRE:
""" + memory_context
                    },
                    {"role": "user", "content": user_message},
                    {"role": "assistant", "content": assistant_response}
                ]
            }
            finetune_data.append(finetune_item)
        
        # √âcrire au format JSONL (chaque ligne est un objet JSON valide)
        with open(output_file, 'w', encoding='utf-8') as f:
            for item in finetune_data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        print(f"‚úÖ {len(finetune_data)} exemples convertis au format d'entra√Ænement dans {output_file}")

def main():
    # Lire la cl√© API de l'environnement si disponible
    api_key = os.environ.get("OPENAI_API_KEY", OPENAI_API_KEY)
    if api_key == "sk-votre_cle_api_ici":
        print("‚ö†Ô∏è Veuillez d√©finir une cl√© API OpenAI valide dans le script ou via la variable d'environnement OPENAI_API_KEY")
        return
    
    # Cr√©er le g√©n√©rateur d'exemples parall√®le
    generator = ParallelGenerator(
        api_key=api_key,
        num_examples=NUM_EXAMPLES,
        batch_size=BATCH_SIZE,
        max_concurrent=MAX_CONCURRENT
    )
    
    # G√©n√©rer les exemples
    examples = generator.run_generation()
    
    # Convertir au format d'entra√Ænement
    generator.process_to_finetune_format()
    
    print(f"üìä Statistiques:")
    print(f"  - Exemples g√©n√©r√©s: {len(examples)}")
    print(f"  - Fichier brut: {OUTPUT_FILE}")
    print(f"  - Fichier format√© pour fine-tuning: {PROCESSED_FILE}")

if __name__ == "__main__":
    main()

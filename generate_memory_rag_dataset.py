#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script pour g√©n√©rer un dataset de fine-tuning DeepSeek optimis√© pour 
la recherche naturelle dans la m√©moire et le RAG.
"""

import os
import json
import time
import random
import threading
from queue import Queue
from concurrent.futures import ThreadPoolExecutor
from openai import OpenAI
import argparse

# Configuration par d√©faut
DEFAULT_OUTPUT_FILE = "./data/memory_rag_dataset.jsonl"
NUM_EXAMPLES = 500
BATCH_SIZE = 10
MAX_CONCURRENT = 5

# Th√®mes vari√©s pour g√©n√©rer des exemples diversifi√©s
TOPICS = [
    "Informatique et programmation",
    "Science et technologie",
    "Histoire et civilisations",
    "√âconomie et finance",
    "Sant√© et m√©decine",
    "Art et litt√©rature",
    "Psychologie et comportement",
    "Philosophie et √©thique",
    "Environnement et d√©veloppement durable",
    "Business et entrepreneuriat",
    "Politique et soci√©t√©"
]

# Types de questions pour varier les exemples
QUESTION_TYPES = [
    "Factuelle simple",
    "Comparative",
    "Explicative",
    "Analytique",
    "Lien entre concepts",
    "Application pratique",
    "R√©sum√© de connaissances",
    "Contraste d'id√©es",
    "Chronologique",
    "D√©finition de concept"
]

class MemoryRAGDatasetGenerator:
    def __init__(self, api_key, output_file, num_examples, batch_size, max_concurrent):
        """Initialise le g√©n√©rateur de dataset pour m√©moire et RAG."""
        self.api_key = api_key
        self.output_file = output_file
        self.num_examples = num_examples
        self.batch_size = batch_size
        self.max_concurrent = max_concurrent
        self.client = OpenAI(api_key=api_key)
        self.examples = []
        self.lock = threading.Lock()
        self.executor = ThreadPoolExecutor(max_workers=max_concurrent)
        
        # Cr√©er le r√©pertoire de sortie si n√©cessaire
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Fichier pour sauvegarder l'avancement
        self.progress_file = os.path.join(os.path.dirname(output_file), "generation_progress.json")
    
    def generate_batch(self, batch_id):
        """G√©n√®re un lot d'exemples pour l'entra√Ænement."""
        current_batch_size = min(self.batch_size, self.num_examples - batch_id * self.batch_size)
        if current_batch_size <= 0:
            return []
        
        topic = random.choice(TOPICS)
        question_type = random.choice(QUESTION_TYPES)
        
        print(f"üîÑ G√©n√©ration du lot {batch_id+1}: {current_batch_size} exemples - Th√®me: {topic} - Type: {question_type}")
        
        prompt = f"""
Tu vas cr√©er {current_batch_size} exemples d'entra√Ænement pour fine-tuner un LLM (DeepSeek) √† chercher intelligemment dans sa m√©moire et son syst√®me RAG.

OBJECTIF: Cr√©er des exemples qui entra√Æneront le LLM √†:
1. Identifier automatiquement quand utiliser sa m√©moire ou le RAG pour r√©pondre
2. Extraire pr√©cis√©ment l'information pertinente des contextes fournis
3. Produire des r√©ponses CONCISES (20-30 mots) bas√©es UNIQUEMENT sur l'information disponible
4. R√©pondre "Information non disponible" quand le contexte ne suffit pas

TH√àME: {topic}
TYPE DE QUESTIONS: {question_type}

FORMAT POUR CHAQUE EXEMPLE:
```
{{
  "conversation_history": [
    {{
      "role": "user", 
      "content": "(question pr√©c√©dente optionnelle)"
    }},
    {{
      "role": "assistant", 
      "content": "(r√©ponse pr√©c√©dente optionnelle)"
    }}
  ],
  "memory_context": "Information stock√©e dans la m√©moire hi√©rarchique du LLM, repr√©sentant des connaissances accumul√©es sur l'utilisateur et des conversations pass√©es",
  "rag_context": "Information extraite d'une base de connaissances externe, comme des articles, documents, ou wikis sur le sujet",
  "current_question": "Question actuelle de l'utilisateur qui n√©cessite de chercher dans les contextes",
  "ideal_concise_answer": "R√©ponse concise et factuelle (20-30 mots) qui utilise UNIQUEMENT l'information des contextes"
}}
```

INSTRUCTIONS:
- Pour chaque exemple, cr√©e un sc√©nario coh√©rent incluant les contextes et la question
- La 'memory_context' doit contenir des informations personnalis√©es comme des pr√©f√©rences ou historiques d'interaction
- Le 'rag_context' doit contenir des informations factuelles ou techniques sur le sujet
- Varie la complexit√© des questions (simples et complexes)
- Inclus des cas o√π l'information n'est PAS disponible dans les contextes
- L'ideal_concise_answer doit √™tre ultra concise (20-30 mots) et factuelle
- IMPORTANT: La r√©ponse doit se baser UNIQUEMENT sur l'information des contextes fournis

Renvoie uniquement un tableau JSON contenant {current_batch_size} exemples dans le format sp√©cifi√©.
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",  # On utilise un mod√®le plus puissant pour g√©n√©rer des exemples de qualit√©
                messages=[
                    {"role": "system", "content": "Tu es un expert en cr√©ation de datasets pour fine-tuning de mod√®les de langage. Tu g√©n√®res des exemples de haute qualit√© pour l'entra√Ænement √† la recherche en m√©moire et RAG."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=4000,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            batch_data = json.loads(content)
            
            # Normaliser le format en cas de diff√©rences dans la structure renvoy√©e
            examples = []
            if isinstance(batch_data, list):
                examples = batch_data
            elif "examples" in batch_data:
                examples = batch_data["examples"]
            else:
                for key, value in batch_data.items():
                    if isinstance(value, dict):
                        examples.append(value)
                    elif isinstance(value, list):
                        examples.extend(value)
            
            # Validation des exemples
            valid_examples = []
            for example in examples:
                # V√©rifier que tous les champs requis sont pr√©sents
                required_fields = ["conversation_history", "memory_context", "rag_context", 
                                  "current_question", "ideal_concise_answer"]
                
                if all(field in example for field in required_fields):
                    # Formater l'exemple pour DeepSeek
                    formatted_example = self._format_for_deepseek(example)
                    valid_examples.append(formatted_example)
                else:
                    missing = [field for field in required_fields if field not in example]
                    print(f"‚ö†Ô∏è Exemple incomplet, champs manquants: {missing}")
            
            print(f"‚úÖ Lot {batch_id+1}: {len(valid_examples)}/{current_batch_size} exemples valides")
            
            # Enregistrer les exemples valides
            with self.lock:
                self.examples.extend(valid_examples)
                self._save_progress()
            
            return valid_examples
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration du lot {batch_id+1}: {str(e)}")
            return []
    
    def _format_for_deepseek(self, example):
        """Convertit un exemple au format requis pour le fine-tuning de DeepSeek."""
        # Construire le prompt syst√®me avec les contextes
        system_prompt = f"""Tu es Ren√©e, une assistante IA francophone qui suit ces r√®gles strictes:
1. R√©ponds UNIQUEMENT en utilisant les informations des CONTEXTES ci-dessous
2. Si l'information n'est pas dans les CONTEXTES, r√©ponds simplement "Information non disponible"
3. Sois TR√àS CONCISE (20-30 mots maximum)
4. Ne g√©n√®re PAS de section <think> - r√©ponds directement
5. Utilise un langage simple et direct

CONTEXTE DE LA M√âMOIRE HI√âRARCHIQUE:
{example.get('memory_context', '')}

CONTEXTE DU SYST√àME DE RAG:
{example.get('rag_context', '')}

RAPPEL: R√©ponds UNIQUEMENT avec les informations pr√©sentes dans les CONTEXTES ci-dessus.
"""
        
        # Formater au format d'entra√Ænement DeepSeek
        formatted_example = {
            "messages": [
                {"role": "system", "content": system_prompt}
            ]
        }
        
        # Ajouter l'historique de conversation si pr√©sent
        conversation_history = example.get('conversation_history', [])
        if conversation_history and isinstance(conversation_history, list):
            formatted_example["messages"].extend(conversation_history)
        
        # Ajouter la question actuelle et la r√©ponse id√©ale
        formatted_example["messages"].append(
            {"role": "user", "content": example.get('current_question', '')}
        )
        formatted_example["messages"].append(
            {"role": "assistant", "content": example.get('ideal_concise_answer', '')}
        )
        
        return formatted_example
    
    def _save_progress(self):
        """Sauvegarde les exemples actuels au format JSONL et l'avancement."""
        # Sauvegarder les exemples au format JSONL
        with open(self.output_file, 'w', encoding='utf-8') as f:
            for example in self.examples:
                f.write(json.dumps(example, ensure_ascii=False) + '\n')
        
        # Sauvegarder le progr√®s
        progress = {
            "total_examples": self.num_examples,
            "generated_examples": len(self.examples),
            "last_update": time.strftime("%Y-%m-%d %H:%M:%S"),
            "percentage_complete": round(len(self.examples) / self.num_examples * 100, 2)
        }
        
        with open(self.progress_file, 'w', encoding='utf-8') as f:
            json.dump(progress, f, indent=2, ensure_ascii=False)
    
    def generate_dataset(self):
        """G√©n√®re l'ensemble du dataset en parall√®le."""
        start_time = time.time()
        print(f"üöÄ D√©marrage de la g√©n√©ration de {self.num_examples} exemples...")
        
        # Calculer le nombre de lots n√©cessaires
        num_batches = (self.num_examples + self.batch_size - 1) // self.batch_size
        
        # Soumettre les t√¢ches de g√©n√©ration
        futures = []
        for batch_id in range(num_batches):
            future = self.executor.submit(self.generate_batch, batch_id)
            futures.append(future)
        
        # Attendre et traiter les r√©sultats
        total_valid_examples = 0
        for future in futures:
            examples = future.result()
            total_valid_examples += len(examples)
        
        # Si nous n'avons pas assez d'exemples, g√©n√©rer des lots suppl√©mentaires
        additional_batch_id = num_batches
        while total_valid_examples < self.num_examples:
            remaining = self.num_examples - total_valid_examples
            print(f"‚ö†Ô∏è Il manque {remaining} exemples, g√©n√©ration de lots suppl√©mentaires...")
            
            future = self.executor.submit(self.generate_batch, additional_batch_id)
            examples = future.result()
            total_valid_examples += len(examples)
            additional_batch_id += 1
        
        # Limiter aux premiers NUM_EXAMPLES si nous en avons trop g√©n√©r√©
        if len(self.examples) > self.num_examples:
            self.examples = self.examples[:self.num_examples]
            self._save_progress()
        
        end_time = time.time()
        duration = end_time - start_time
        print(f"‚ú® G√©n√©ration termin√©e! {len(self.examples)}/{self.num_examples} exemples g√©n√©r√©s en {duration:.2f} secondes")
        print(f"üìä Vitesse moyenne: {len(self.examples) / duration:.2f} exemples/seconde")
        print(f"üíæ Dataset sauvegard√© dans: {self.output_file}")
        
        return self.examples

def main():
    parser = argparse.ArgumentParser(description='G√©n√©rateur de dataset pour fine-tuning DeepSeek sur m√©moire et RAG')
    parser.add_argument('--api_key', type=str, help='Cl√© API OpenAI')
    parser.add_argument('--output', type=str, default=DEFAULT_OUTPUT_FILE, help='Fichier de sortie au format JSONL')
    parser.add_argument('--num_examples', type=int, default=NUM_EXAMPLES, help='Nombre d\'exemples √† g√©n√©rer')
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE, help='Taille des lots pour la g√©n√©ration')
    parser.add_argument('--concurrent', type=int, default=MAX_CONCURRENT, help='Nombre de t√¢ches parall√®les')
    
    args = parser.parse_args()
    
    # R√©cup√©rer la cl√© API soit des arguments, soit des variables d'environnement
    api_key = args.api_key or os.environ.get("OPENAI_API_KEY")
    
    if not api_key:
        print("‚ùå Erreur: Cl√© API OpenAI requise. Utilisez --api_key ou d√©finissez la variable d'environnement OPENAI_API_KEY")
        return
    
    # Initialiser et lancer le g√©n√©rateur
    generator = MemoryRAGDatasetGenerator(
        api_key=api_key,
        output_file=args.output,
        num_examples=args.num_examples,
        batch_size=args.batch_size,
        max_concurrent=args.concurrent
    )
    
    generator.generate_dataset()

if __name__ == "__main__":
    main()
